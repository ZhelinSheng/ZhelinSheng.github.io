<!doctype html>

<html lang="en">

	<head>
		<meta charset="utf-8">
		<title>Lab 8 - Principal component analysis</title>
		<link rel="stylesheet" href="css/styles.css">
		<link rel="stylesheet" href="css/navigation.css">
	</head>

	<body>
	
	
	<div class="container">
	
	<header>
	
		<h1> Principal component analysis </h1>
		
		
		<div> This is a data analysis topic I’m particularly interested in, we also did some refinement of it with my research group. We even write a paper discussing it.[2]</div>
		
	</header>
	
	<nav class="menu">
	<ul>
		<li><a href="about-pca.html">About PCA</a></li>
		<li><a href="introduction.html">Introduction</a></li>
		<li><a href="math-interpretation.html" class="is-current">Math interpretation</a></li>
		<li><a href="fractal-phenomenon.html">Fractal phenomenon</a></li>
		<li><a href="some-results.html">Some results</a></li>
		<li><a href="further-interest.html">Further interest</a></li>
	</ul>
	
	</nav>
	
	
	<article>
		
		<section>
		<h2>Math interpretation</h2>
			<p>The principal components of a collection of points in a real coordinate space are a sequence of p unit vectors, where the i-th vector is the direction of a line that best fits the data while being orthogonal to the first i-1 vectors. 
			Here, a best-fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line. These directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. 
			Principal component analysis is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest. </p>
			
			<p>the process of PCA: suppose there is a data set X = {x1, x2, x3, ... xn}, xi ∈ R^D and we want to reduce its dimension to a lower one without losing too much of the information.
			Firstly, the data set needs to be centralized for an easier covariance matrix calculation later, that is, for each component xp (1 ≤ p ≤ D) in R^D, we subtract the coordinate of each point in component xp by the average of the sum of all points’ corresponding coordinates.
			</p>
			
			<p>Then we find the data covariance matrix XX^T, calculating the m-th largest eigenvalues of that covariance matrix, and thus we can get the m corresponding eigenvectors w1, w2, ...wm, named as m principle components. 
			After generating a new matrix W = {w1, w2, w3, ....wm}, wi ∈ R^D, the compressed data becomes yi = W^T xi for each xi. We reduce the dimension of data set to X′ = {y1, y2, y3, .... yn}, yi ∈ R^M. </p>
		</section>
		
		
		
		
		
	</article>
	
	
	<aside>
	
		<h2>Other applications</h2>
			<p>Besides the one mentioned above, as our research focuses on, 
			there are also a list of PCA’s applications on other fields and many Software or source code that already implements PCA and truncated PCA. </p>
			
			
			
			<h3>Applications </h3>
			<ul class="app-list">
				<li>Intelligence</li>
				<li class="different">Residential differentiation</li>
				<li >Development indexes</li>
				<li class="different">Population genetics</li>
				<li>Market research and indexes of attitude</li>
				<li>Quantitative finance</li>
				<li>Neuroscience</li>
				<li>Factor analysis</li>
				<li>Iconography of correlations</li>
			</ul>
			
			<h3>Software that supports PCA </h3>
			<div> sorted by first letter, from a to z(not case-sensitive).</div>
			<ol class="soft-list">
				<li class="different">ALGLIB</li>
				<li>Analytica</li>
				<li class="different">ELKI</li>
				<li class="different">Gretl</li>
				<li>MATLAB</li>
				<li>Matplotlib</li>
				<li>R</li>
				<li>scikit-learn</li>
				<li>Oracle Database</li>
			</ol>
			
		
	
	</aside>
	
		

	</div>
	</body>

</html>